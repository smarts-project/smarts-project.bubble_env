Updates from last version:
1. Add PPO algorithm implementation from an open-source RL training framework PARL
you can run algorithm simply in command `python train.py`
2. Clean code and regenalize jsonfile of scenarios. You can run example in `bubble_env/visualize_example.py`
3. There have been some changes in `core.py` and `env_wrapper.py` for bubble control logic.
4. You may need to change `agent_interface` and `social_interface` for competition setting.